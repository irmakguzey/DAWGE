{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data \n",
    "\n",
    "from copy import deepcopy\n",
    "from omegaconf import OmegaConf\n",
    "from torchvision import transforms\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# \n",
    "from contrastive_learning.tests.test_model import load_lin_model, predict_traj_actions\n",
    "from contrastive_learning.tests.animate_markers import AnimateMarkers\n",
    "from contrastive_learning.tests.animate_rvec_tvec import AnimateRvecTvec\n",
    "from contrastive_learning.datasets.dataloaders import get_dataloaders\n",
    "\n",
    "from contrastive_learning.models.custom_models import LinearInverse\n",
    "from contrastive_learning.datasets.state_dataset import StateDataset\n",
    "from contrastive_learning.tests.plotting import plot_rvec_tvec, plot_corners\n",
    "from contrastive_learning.datasets.dataloaders import get_dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading\n",
    "Create the distributed group\n",
    "Load the linear inverse model from the saved path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the multiprocessing to load the saved models properly\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29505\"\n",
    "\n",
    "torch.distributed.init_process_group(backend='gloo', rank=0, world_size=1)\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device and out_dir\n",
    "device = torch.device('cuda:0')\n",
    "out_dir = '/home/irmak/Workspace/DAWGE/contrastive_learning/out/2022.08.01/20-14_pli_ref_dog_lf_mse_fi_1_pt_corners_bs_64_hd_64_lr_0.001_zd_8'\n",
    "cfg = OmegaConf.load(os.path.join(out_dir, '.hydra/config.yaml'))\n",
    "if not ('pos_ref' in cfg):\n",
    "    cfg.pos_ref = 'global'\n",
    "model_path = os.path.join(out_dir, 'models/lin_model.pt')\n",
    "\n",
    "# Load the encoder\n",
    "lin_model = load_lin_model(cfg, device, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistributedDataParallel(\n",
      "  (module): LinearInverse(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=16, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lin_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'_target_': 'contrastive_learning.models.agents.pli.PLI', 'loss_fn': 'mse', 'use_encoder': False, 'model': '???', 'optimizer': '???'}, 'optimizer': {'_target_': 'torch.optim.Adam', 'params': '???', 'lr': '???', 'weight_decay': '???'}, 'model': {'_target_': 'contrastive_learning.models.custom_models.LinearInverse', 'input_dim': '???', 'action_dim': '???', 'hidden_dim': '???'}, 'pos_encoder': {'_target_': 'contrastive_learning.models.custom_models.PosToEmbedding', 'input_dim': '???', 'hidden_dim': '???', 'out_dim': '???'}, 'seed': 42, 'device': 'cuda', 'agent_type': 'pli', 'dataset_type': 'state', 'pos_type': 'corners', 'pos_ref': 'dog', 'train_epochs': 1000, 'save_frequency': 10, 'train_dset_split': 0.8, 'batch_size': 64, 'lr': 0.001, 'weight_decay': 1e-05, 'z_dim': 8, 'pos_dim': 8, 'hidden_dim': 64, 'action_dim': 2, 'distributed': True, 'num_workers': 4, 'world_size': 1, 'num_gpus': 4, 'fps': 15, 'frame_interval': 1, 'video_type': 'color', 'experiment': '${agent_type}_ref_${pos_ref}_lf_${agent.loss_fn}_fi_${frame_interval}_pt_${pos_type}_bs_${batch_size}_hd_${hidden_dim}_lr_${lr}_zd_${z_dim}', 'data_dir': '/home/irmak/Workspace/DAWGE/src/dawge_planner/data/box_orientation_2_demos/train_demos', 'checkpoint_dir': '???', 'log_frequency': 1}\n"
     ]
    }
   ],
   "source": [
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Animation\n",
    "1. Dump every predicted action for given data directory\n",
    "2. Save the predicted and current action in a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_name = 'box_marker_test_6'\n",
    "data_dir = '/home/irmak/Workspace/DAWGE/src/dawge_planner/data/box_orientation_2_demos/test_demos/{}'.format(demo_name)\n",
    "dump_dir = '/home/irmak/Workspace/DAWGE/contrastive_learning/tests/animations'\n",
    "exp_name = '{}_{}'.format(out_dir.split('/')[-2], out_dir.split('/')[-1])\n",
    "dump_file = '{}_{}.mp4'.format(demo_name, exp_name)\n",
    "\n",
    "fps = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump_file: box_marker_test_6_2022.08.01_20-14_pli_ref_dog_lf_mse_fi_1_pt_corners_bs_64_hd_64_lr_0.001_zd_8.mp4\n"
     ]
    }
   ],
   "source": [
    "print('dump_file: {}'.format(dump_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_curr_state(cfg, data_loader_iter):\n",
    "    batch = next(data_loader_iter)\n",
    "    curr_pos, next_pos, action = [b for b in batch]\n",
    "    action = all_dset.denormalize_action(action[0].detach().numpy())\n",
    "    if cfg.pos_type == 'corners':\n",
    "        curr_pos, next_pos = all_dset.denormalize_corner(curr_pos[0].detach().numpy()), all_dset.denormalize_corner(next_pos[0].detach().numpy())\n",
    "    elif cfg.pos_type == 'rvec_tvec':\n",
    "        curr_pos, next_pos = all_dset.denormalize_pos_rvec_tvec(curr_pos[0].detach().numpy()), all_dset.denormalize_pos_rvec_tvec(next_pos[0].detach().numpy())\n",
    "        \n",
    "    return curr_pos, next_pos, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_pos_to_model(cfg, curr_pos): # Returns flattened positions\n",
    "    if cfg.pos_type == 'corners':\n",
    "        curr_pos = torch.FloatTensor(all_dset.normalize_corner(curr_pos))\n",
    "        return torch.flatten(curr_pos)\n",
    "    elif cfg.pos_type == 'rvec_tvec':\n",
    "        curr_pos = torch.FloatTensor(all_dset.normalize_rvec_tvec(curr_pos))\n",
    "        return curr_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference(cfg, curr_pos): # Gets flattened positions, shape: (16)\n",
    "    ref_tensor = torch.zeros((curr_pos.shape))\n",
    "    half_idx = int(curr_pos.shape[0] / 2) # In order not to have a control for pos_type\n",
    "    if cfg.pos_ref == 'dog':\n",
    "        ref_tensor = curr_pos[half_idx:]\n",
    "        ref_tensor = ref_tensor.repeat(1,2)\n",
    "    elif cfg.pos_ref == 'box':\n",
    "        ref_tensor = curr_pos[:half_idx]\n",
    "        ref_tensor = ref_tensor.repeat(1,2)\n",
    "    return ref_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET POS_REF: global\n",
      "len(dataset): 64\n",
      "self.action_min: [ 0.         -0.30000001], self.action_max: [0.15000001 0.30000001]\n",
      "DATASET POS_REF: global\n",
      "len(dataset): 2683\n",
      "self.action_min: [-0.15000001 -0.30000001], self.action_max: [0.15000001 0.30000001]\n"
     ]
    }
   ],
   "source": [
    "# Get the dataset\n",
    "global_cfg = deepcopy(cfg)\n",
    "global_cfg.pos_ref = 'global'\n",
    "dataset = StateDataset(global_cfg, single_dir=True, single_dir_root=data_dir)\n",
    "test_loader = data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "test_loader_iter = iter(test_loader)\n",
    "_, _, all_dset = get_dataloaders(global_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dataset)\n",
    "\n",
    "curr_traj = np.zeros((N, cfg.pos_dim,2))\n",
    "next_traj = np.zeros((N, cfg.pos_dim,2))\n",
    "real_actions = np.zeros((N, 2))\n",
    "pred_actions = np.zeros((N, 2))\n",
    "\n",
    "for i in range(N):\n",
    "    curr_pos, next_pos, action = get_curr_state(cfg, test_loader_iter)\n",
    "    real_actions[i,:] = action\n",
    "    curr_traj[i,:] = curr_pos\n",
    "    next_traj[i,:] = next_pos\n",
    "    \n",
    "    # Normalize the current and next pos before inputting to the linear model\n",
    "    curr_pos, next_pos = trans_pos_to_model(cfg, curr_pos).to(device), trans_pos_to_model(cfg, next_pos).to(device)\n",
    "    ref_tensor = get_reference(cfg, curr_pos)\n",
    "    \n",
    "    pred_action = lin_model(curr_pos-ref_tensor, next_pos-ref_tensor)\n",
    "    pred_action = all_dset.denormalize_action(pred_action[0].cpu().detach().numpy())\n",
    "    \n",
    "    pred_actions[i,:] = pred_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 16\n",
      "(4, 16)\n"
     ]
    }
   ],
   "source": [
    "nrows = 4\n",
    "ncols = math.ceil(N / nrows)\n",
    "print(N, ncols)\n",
    "fig, axs = plt.subplots(figsize=(ncols*10,nrows*10), nrows=nrows, ncols=ncols) # Draw the predicted action\n",
    "print(axs.shape)\n",
    "fig.suptitle(\"Frame-by-frame Predictions\")\n",
    "\n",
    "for i in range(N):\n",
    "    axs_row = int(i / ncols)\n",
    "    axs_col = int(i % ncols)\n",
    "\n",
    "    real_action = real_actions[i]\n",
    "    pred_action = pred_actions[i]\n",
    "    \n",
    "    curr_pos = curr_traj[i]\n",
    "    next_pos = next_traj[i]\n",
    "    \n",
    "    axs[axs_row,axs_col].set_title(\"Frame: {}\".format(i))\n",
    "    if cfg.pos_type == 'corners':\n",
    "        _, frame_axis = plot_corners(axs[axs_row,axs_col], curr_pos, plot_action=True, actions=[real_action, pred_action], color_scheme=1)\n",
    "        plot_corners(axs[axs_row,axs_col], next_pos, plot_action=False, use_frame_axis=True, frame_axis=frame_axis, color_scheme=2)\n",
    "    elif cfg.pos_type == 'rvec_tvec':\n",
    "        _, frame_axis = plot_rvec_tvec(axs[axs_row,axs_col], curr_pos, plot_action=True, actions=[real_action], color_scheme=1)\n",
    "        plot_rvec_tvec(axs[axs_row,axs_col], next_pos, plot_action=True, actions=[pred_action], use_frame_axis=True, frame_axis=frame_axis, color_scheme=2)\n",
    "\n",
    "plt_file_name = '{}_{}.png'.format(demo_name, exp_name)\n",
    "plt.savefig(os.path.join('/home/irmak/Workspace/DAWGE/contrastive_learning/tests/plots', plt_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset\n",
    "# global_cfg = deepcopy(cfg)\n",
    "# global_cfg.pos_ref = 'global'\n",
    "# dataset = StateDataset(global_cfg, single_dir=True, single_dir_root=data_dir)\n",
    "# predicted_actions = np.zeros((len(dataset), 2))\n",
    "# test_loader = data.DataLoader(dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# for i, batch in enumerate(test_loader):\n",
    "#     curr_pos, next_pos, action = [b.to(device) for b in batch]\n",
    "    \n",
    "#     # Normalize the current and next pos before inputting to the linear model\n",
    "#     ref_tensor = torch.zeros((curr_pos.shape))\n",
    "#     half_idx = int(curr_pos.shape[1] / 2) # In order not to have a control for pos_type\n",
    "#     if cfg.pos_ref == 'dog':\n",
    "#         ref_tensor = curr_pos[:,half_idx:]\n",
    "#         ref_tensor = ref_tensor.repeat(1,2)\n",
    "#     elif cfg.pos_ref == 'box':\n",
    "#         ref_tensor = curr_pos[:,:half_idx]\n",
    "#         ref_tensor = ref_tensor.repeat(1,2)\n",
    "    \n",
    "#     pred_action = lin_model(curr_pos-ref_tensor, next_pos-ref_tensor)\n",
    "\n",
    "#     print('Actual Action \\t Predicted Action')\n",
    "#     for j in range(len(action)):\n",
    "#         print('{}, \\t{}'.format(np.around(dataset.denormalize_action(action[j][0].cpu().detach().numpy()), 2),\n",
    "#                                           dataset.denormalize_action(pred_action[j][0].cpu().detach().numpy())))\n",
    "#         predicted_actions[i*cfg.batch_size+j,:] = dataset.denormalize_action(pred_action[j][0].cpu().detach().numpy())\n",
    "\n",
    "# with open(os.path.join(data_dir, 'predicted_actions.npy'), 'wb') as f:\n",
    "#     np.save(f, predicted_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if cfg.pos_type == 'corners':\n",
    "#     AnimateMarkers(\n",
    "#         data_dir = data_dir, \n",
    "#         dump_dir = dump_dir, \n",
    "#         dump_file = dump_file, \n",
    "#         fps = fps,\n",
    "#         mult_traj = False,\n",
    "#         show_predicted_action = True \n",
    "#     ) # Saves them in the given dump_file\n",
    "# elif cfg.pos_type == 'rvec_tvec':\n",
    "#     AnimateRvecTvec(\n",
    "#         data_dir = data_dir, \n",
    "#         dump_dir = dump_dir, \n",
    "#         dump_file = dump_file,\n",
    "#         fps = fps,\n",
    "#         show_predicted_action=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Prediction Image\n",
    "Predict the action for each frame in the test dataset and dump them in a grid image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET POS_REF: global\n",
      "len(dataset): 2613\n",
      "self.action_min: [-0.15000001 -0.30000001], self.action_max: [0.15000001 0.30000001]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "cfg.batch_size = 32\n",
    "global_cfg = deepcopy(cfg)\n",
    "global_cfg.pos_ref = 'global'\n",
    "train_loader, test_loader, dataset = get_dataloaders(global_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_pos: tensor([[0.5562, 0.5154, 0.5277, 0.4888, 0.5472, 0.4580, 0.5757, 0.4832, 0.6181,\n",
      "         0.5098, 0.6474, 0.4972, 0.6653, 0.5322, 0.6352, 0.5462],\n",
      "        [0.1600, 0.5091, 0.1592, 0.4692, 0.1967, 0.4573, 0.1987, 0.4972, 0.1287,\n",
      "         0.6022, 0.1686, 0.5896, 0.1686, 0.6359, 0.1254, 0.6471],\n",
      "        [0.4691, 0.1653, 0.4910, 0.1569, 0.5041, 0.1793, 0.4813, 0.1877, 0.4381,\n",
      "         0.1877, 0.4503, 0.2073, 0.4267, 0.2157, 0.4145, 0.1947],\n",
      "        [0.2769, 0.1751, 0.2557, 0.1975, 0.2362, 0.1807, 0.2573, 0.1597, 0.2834,\n",
      "         0.1555, 0.2638, 0.1429, 0.2818, 0.1218, 0.3013, 0.1345],\n",
      "        [0.6832, 0.1036, 0.7068, 0.1064, 0.7109, 0.1275, 0.6865, 0.1232, 0.6539,\n",
      "         0.1120, 0.6564, 0.1331, 0.6327, 0.1275, 0.6311, 0.1078],\n",
      "        [0.3029, 0.1485, 0.2761, 0.1597, 0.2687, 0.1373, 0.2948, 0.1289, 0.3156,\n",
      "         0.1127, 0.3217, 0.0931, 0.3457, 0.0945, 0.3396, 0.1134],\n",
      "        [0.1702, 0.5196, 0.1653, 0.4790, 0.2036, 0.4636, 0.2093, 0.5028, 0.0651,\n",
      "         0.8067, 0.0936, 0.8445, 0.0497, 0.8992, 0.0220, 0.8585],\n",
      "        [0.1441, 0.5182, 0.1498, 0.4748, 0.1873, 0.4692, 0.1832, 0.5126, 0.1205,\n",
      "         0.5924, 0.1547, 0.6092, 0.1303, 0.6555, 0.0945, 0.6373],\n",
      "        [0.5432, 0.1296, 0.5664, 0.1254, 0.5757, 0.1471, 0.5521, 0.1506, 0.5130,\n",
      "         0.1471, 0.5187, 0.1695, 0.4943, 0.1709, 0.4894, 0.1471],\n",
      "        [0.2557, 0.2815, 0.2256, 0.2969, 0.2142, 0.2689, 0.2443, 0.2549, 0.1604,\n",
      "         0.2843, 0.1816, 0.3039, 0.1564, 0.3249, 0.1352, 0.3053],\n",
      "        [0.1686, 0.5168, 0.1653, 0.4762, 0.2036, 0.4636, 0.2077, 0.5014, 0.1490,\n",
      "         0.5966, 0.1881, 0.5980, 0.1767, 0.6471, 0.1360, 0.6429],\n",
      "        [0.6580, 0.4020, 0.6824, 0.4300, 0.6604, 0.4538, 0.6360, 0.4244, 0.6401,\n",
      "         0.6134, 0.6376, 0.6625, 0.6010, 0.6471, 0.6050, 0.5994],\n",
      "        [0.3542, 0.1723, 0.3420, 0.1415, 0.3664, 0.1317, 0.3787, 0.1625, 0.2809,\n",
      "         0.1190, 0.2964, 0.1345, 0.2744, 0.1471, 0.2590, 0.1317],\n",
      "        [0.2085, 0.3641, 0.2199, 0.3319, 0.2516, 0.3375, 0.2419, 0.3697, 0.1963,\n",
      "         0.4258, 0.2296, 0.4328, 0.2150, 0.4678, 0.1800, 0.4594],\n",
      "        [0.2500, 0.2381, 0.2663, 0.2143, 0.2923, 0.2255, 0.2769, 0.2493, 0.2305,\n",
      "         0.2843, 0.2573, 0.2969, 0.2394, 0.3207, 0.2125, 0.3081],\n",
      "        [0.3819, 0.2087, 0.4064, 0.1975, 0.4202, 0.2185, 0.3966, 0.2325, 0.3542,\n",
      "         0.2381, 0.3681, 0.2619, 0.3420, 0.2745, 0.3282, 0.2479],\n",
      "        [0.1572, 0.5252, 0.1531, 0.4846, 0.1914, 0.4692, 0.1963, 0.5084, 0.1287,\n",
      "         0.6092, 0.1686, 0.6064, 0.1580, 0.6569, 0.1164, 0.6569],\n",
      "        [0.1661, 0.5154, 0.1653, 0.4734, 0.2036, 0.4622, 0.2052, 0.5028, 0.1637,\n",
      "         0.6232, 0.2044, 0.6176, 0.1987, 0.6667, 0.1555, 0.6723],\n",
      "        [0.2207, 0.3782, 0.2362, 0.3473, 0.2679, 0.3571, 0.2533, 0.3880, 0.1906,\n",
      "         0.4398, 0.2191, 0.4580, 0.1963, 0.4916, 0.1661, 0.4720],\n",
      "        [0.4642, 0.3445, 0.4365, 0.3627, 0.4218, 0.3333, 0.4495, 0.3179, 0.5440,\n",
      "         0.5784, 0.5529, 0.5406, 0.5871, 0.5518, 0.5790, 0.5910],\n",
      "        [0.2321, 0.2787, 0.2003, 0.2815, 0.2036, 0.2507, 0.2353, 0.2493, 0.1254,\n",
      "         0.2759, 0.1547, 0.2801, 0.1433, 0.3081, 0.1124, 0.3025],\n",
      "        [0.4064, 0.2101, 0.4210, 0.1905, 0.4438, 0.2031, 0.4300, 0.2241, 0.3958,\n",
      "         0.2549, 0.4194, 0.2703, 0.4039, 0.2913, 0.3795, 0.2745],\n",
      "        [0.3062, 0.2773, 0.2744, 0.2857, 0.2695, 0.2577, 0.2989, 0.2493, 0.2256,\n",
      "         0.2549, 0.2345, 0.2801, 0.2036, 0.2927, 0.1954, 0.2647],\n",
      "        [0.3730, 0.2535, 0.3469, 0.2703, 0.3331, 0.2451, 0.3583, 0.2297, 0.4194,\n",
      "         0.2815, 0.4194, 0.2549, 0.4471, 0.2507, 0.4479, 0.2773],\n",
      "        [0.4853, 0.0980, 0.4967, 0.0826, 0.5179, 0.0938, 0.5073, 0.1092, 0.4748,\n",
      "         0.1373, 0.4910, 0.1513, 0.4739, 0.1639, 0.4568, 0.1485],\n",
      "        [0.2663, 0.3515, 0.2980, 0.3361, 0.3119, 0.3641, 0.2801, 0.3824, 0.2280,\n",
      "         0.3137, 0.2231, 0.3473, 0.1897, 0.3459, 0.1954, 0.3123],\n",
      "        [0.6645, 0.0350, 0.6832, 0.0308, 0.6946, 0.0462, 0.6759, 0.0504, 0.6433,\n",
      "         0.0574, 0.6547, 0.0728, 0.6360, 0.0756, 0.6254, 0.0602],\n",
      "        [0.1694, 0.5077, 0.1649, 0.4692, 0.2024, 0.4538, 0.2085, 0.4916, 0.1433,\n",
      "         0.5868, 0.1824, 0.5826, 0.1743, 0.6303, 0.1327, 0.6345],\n",
      "        [0.6181, 0.1569, 0.6352, 0.1443, 0.6531, 0.1597, 0.6376, 0.1751, 0.5985,\n",
      "         0.1849, 0.6173, 0.2045, 0.6002, 0.2157, 0.5822, 0.1961],\n",
      "        [0.1637, 0.5238, 0.1572, 0.4832, 0.1954, 0.4664, 0.2028, 0.5042, 0.2134,\n",
      "         0.6303, 0.2402, 0.5854, 0.2712, 0.6036, 0.2459, 0.6499],\n",
      "        [0.1710, 0.5140, 0.1694, 0.4720, 0.2077, 0.4608, 0.2093, 0.5014, 0.1254,\n",
      "         0.7605, 0.1678, 0.7647, 0.1515, 0.8263, 0.1067, 0.8207],\n",
      "        [0.1922, 0.5042, 0.1971, 0.4636, 0.2353, 0.4580, 0.2313, 0.4986, 0.1596,\n",
      "         0.5756, 0.1979, 0.5784, 0.1849, 0.6261, 0.1450, 0.6204]],\n",
      "       device='cuda:0'), next_pos: tensor([[0.5562, 0.5154, 0.5277, 0.4888, 0.5464, 0.4594, 0.5757, 0.4832, 0.6140,\n",
      "         0.5182, 0.6409, 0.5000, 0.6629, 0.5308, 0.6368, 0.5518],\n",
      "        [0.1653, 0.4930, 0.1637, 0.4552, 0.2011, 0.4426, 0.2036, 0.4804, 0.1393,\n",
      "         0.5742, 0.1800, 0.5672, 0.1735, 0.6120, 0.1327, 0.6176],\n",
      "        [0.4796, 0.1555, 0.5033, 0.1499, 0.5130, 0.1709, 0.4902, 0.1779, 0.4454,\n",
      "         0.1751, 0.4585, 0.1961, 0.4349, 0.2045, 0.4226, 0.1835],\n",
      "        [0.2647, 0.1849, 0.2463, 0.2087, 0.2239, 0.1954, 0.2423, 0.1723, 0.2679,\n",
      "         0.1639, 0.2484, 0.1513, 0.2663, 0.1303, 0.2858, 0.1415],\n",
      "        [0.6979, 0.1022, 0.7207, 0.1064, 0.7248, 0.1261, 0.7011, 0.1218, 0.6686,\n",
      "         0.1106, 0.6710, 0.1307, 0.6479, 0.1256, 0.6458, 0.1060],\n",
      "        [0.2956, 0.1457, 0.2704, 0.1583, 0.2590, 0.1387, 0.2834, 0.1261, 0.3070,\n",
      "         0.1106, 0.3135, 0.0910, 0.3371, 0.0924, 0.3306, 0.1106],\n",
      "        [0.1694, 0.5182, 0.1653, 0.4790, 0.2036, 0.4636, 0.2093, 0.5028, 0.0871,\n",
      "         0.7689, 0.1132, 0.8039, 0.0717, 0.8543, 0.0440, 0.8165],\n",
      "        [0.1539, 0.4986, 0.1612, 0.4580, 0.1979, 0.4538, 0.1930, 0.4958, 0.1303,\n",
      "         0.5728, 0.1645, 0.5882, 0.1409, 0.6345, 0.1059, 0.6162],\n",
      "        [0.5529, 0.1289, 0.5765, 0.1261, 0.5847, 0.1485, 0.5603, 0.1499, 0.5244,\n",
      "         0.1443, 0.5301, 0.1667, 0.5057, 0.1667, 0.5008, 0.1443],\n",
      "        [0.2638, 0.2787, 0.2337, 0.2955, 0.2223, 0.2689, 0.2524, 0.2549, 0.1694,\n",
      "         0.2829, 0.1906, 0.3025, 0.1645, 0.3235, 0.1441, 0.3039],\n",
      "        [0.1710, 0.5042, 0.1718, 0.4622, 0.2093, 0.4524, 0.2101, 0.4930, 0.1498,\n",
      "         0.5798, 0.1897, 0.5812, 0.1783, 0.6275, 0.1376, 0.6261],\n",
      "        [0.6580, 0.4020, 0.6824, 0.4300, 0.6604, 0.4538, 0.6360, 0.4244, 0.6352,\n",
      "         0.5896, 0.6409, 0.6359, 0.6050, 0.6331, 0.6002, 0.5854],\n",
      "        [0.3583, 0.1807, 0.3453, 0.1471, 0.3689, 0.1359, 0.3819, 0.1681, 0.2858,\n",
      "         0.1218, 0.2980, 0.1401, 0.2736, 0.1499, 0.2622, 0.1317],\n",
      "        [0.2191, 0.3529, 0.2288, 0.3207, 0.2606, 0.3249, 0.2516, 0.3571, 0.2036,\n",
      "         0.4132, 0.2345, 0.4244, 0.2158, 0.4580, 0.1840, 0.4440],\n",
      "        [0.2647, 0.2283, 0.2793, 0.2045, 0.3054, 0.2157, 0.2915, 0.2381, 0.2410,\n",
      "         0.2731, 0.2655, 0.2885, 0.2451, 0.3123, 0.2207, 0.2941],\n",
      "        [0.3966, 0.1989, 0.4210, 0.1877, 0.4349, 0.2087, 0.4112, 0.2213, 0.3689,\n",
      "         0.2283, 0.3827, 0.2521, 0.3575, 0.2633, 0.3436, 0.2395],\n",
      "        [0.1621, 0.5042, 0.1621, 0.4650, 0.1995, 0.4538, 0.2011, 0.4930, 0.1336,\n",
      "         0.5840, 0.1726, 0.5826, 0.1629, 0.6289, 0.1213, 0.6303],\n",
      "        [0.1661, 0.5154, 0.1653, 0.4734, 0.2036, 0.4622, 0.2052, 0.5028, 0.1702,\n",
      "         0.6008, 0.2085, 0.6036, 0.1963, 0.6527, 0.1555, 0.6485],\n",
      "        [0.2215, 0.3725, 0.2410, 0.3417, 0.2695, 0.3557, 0.2508, 0.3866, 0.1922,\n",
      "         0.4258, 0.2215, 0.4412, 0.1995, 0.4748, 0.1702, 0.4580],\n",
      "        [0.4642, 0.3445, 0.4365, 0.3627, 0.4218, 0.3333, 0.4495, 0.3179, 0.5212,\n",
      "         0.5560, 0.5309, 0.5182, 0.5643, 0.5294, 0.5554, 0.5686],\n",
      "        [0.2410, 0.2801, 0.2085, 0.2857, 0.2093, 0.2549, 0.2402, 0.2507, 0.1368,\n",
      "         0.2759, 0.1645, 0.2857, 0.1466, 0.3123, 0.1189, 0.3011],\n",
      "        [0.4218, 0.2003, 0.4332, 0.1807, 0.4577, 0.1905, 0.4471, 0.2115, 0.4137,\n",
      "         0.2465, 0.4332, 0.2661, 0.4137, 0.2829, 0.3933, 0.2619],\n",
      "        [0.3103, 0.2787, 0.2801, 0.2927, 0.2712, 0.2633, 0.3013, 0.2535, 0.2337,\n",
      "         0.2661, 0.2370, 0.2955, 0.2052, 0.3011, 0.2028, 0.2703],\n",
      "        [0.3738, 0.2535, 0.3469, 0.2689, 0.3331, 0.2451, 0.3591, 0.2297, 0.4194,\n",
      "         0.2941, 0.4145, 0.2661, 0.4422, 0.2577, 0.4471, 0.2857],\n",
      "        [0.4938, 0.0913, 0.5060, 0.0770, 0.5262, 0.0888, 0.5147, 0.1034, 0.4821,\n",
      "         0.1246, 0.5008, 0.1401, 0.4845, 0.1527, 0.4666, 0.1359],\n",
      "        [0.2655, 0.3641, 0.2948, 0.3445, 0.3127, 0.3725, 0.2842, 0.3936, 0.2264,\n",
      "         0.3305, 0.2158, 0.3641, 0.1824, 0.3557, 0.1946, 0.3221],\n",
      "        [0.6730, 0.0294, 0.6918, 0.0259, 0.7032, 0.0413, 0.6844, 0.0448, 0.6547,\n",
      "         0.0490, 0.6653, 0.0644, 0.6482, 0.0672, 0.6368, 0.0518],\n",
      "        [0.1808, 0.4846, 0.1775, 0.4468, 0.2142, 0.4342, 0.2191, 0.4706, 0.1547,\n",
      "         0.5686, 0.1922, 0.5742, 0.1767, 0.6204, 0.1376, 0.6134],\n",
      "        [0.6319, 0.1485, 0.6490, 0.1359, 0.6669, 0.1527, 0.6515, 0.1653, 0.6120,\n",
      "         0.1779, 0.6303, 0.1961, 0.6136, 0.2080, 0.5953, 0.1884],\n",
      "        [0.1637, 0.5238, 0.1572, 0.4832, 0.1954, 0.4664, 0.2028, 0.5042, 0.1946,\n",
      "         0.6261, 0.2215, 0.5812, 0.2533, 0.5994, 0.2264, 0.6443],\n",
      "        [0.1710, 0.5140, 0.1694, 0.4720, 0.2077, 0.4608, 0.2093, 0.5014, 0.1360,\n",
      "         0.7157, 0.1775, 0.7199, 0.1621, 0.7773, 0.1181, 0.7731],\n",
      "        [0.2036, 0.4748, 0.2125, 0.4370, 0.2484, 0.4384, 0.2402, 0.4776, 0.1694,\n",
      "         0.5434, 0.2068, 0.5462, 0.1954, 0.5896, 0.1555, 0.5868]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader.dataset)\n",
    "batch = next(iter(test_loader))\n",
    "curr_pos, next_pos, action = [b.to(device) for b in batch]\n",
    "print('curr_pos: {}, next_pos: {}'.format(curr_pos, next_pos))\n",
    "\n",
    "# Normalize the current and next pos before inputting to the linear model\n",
    "ref_tensor = torch.zeros((curr_pos.shape))\n",
    "half_idx = int(curr_pos.shape[1] / 2) # In order not to have a control for pos_type\n",
    "if cfg.pos_ref == 'dog':\n",
    "    ref_tensor = curr_pos[:,half_idx:]\n",
    "    ref_tensor = ref_tensor.repeat(1,2)\n",
    "elif cfg.pos_ref == 'box':\n",
    "    ref_tensor = curr_pos[:,:half_idx]\n",
    "    ref_tensor = ref_tensor.repeat(1,2)\n",
    "\n",
    "pred_action = lin_model(curr_pos-ref_tensor, next_pos-ref_tensor)\n",
    "\n",
    "curr_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5562, 0.5154, 0.5277, 0.4888, 0.5472, 0.4580, 0.5757, 0.4832, 0.6181,\n",
      "        0.5098, 0.6474, 0.4972, 0.6653, 0.5322, 0.6352, 0.5462],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(curr_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8)\n"
     ]
    }
   ],
   "source": [
    "nrows = 4\n",
    "ncols = int(cfg.batch_size / nrows)\n",
    "fig, axs = plt.subplots(figsize=(ncols*10,nrows*10), nrows=nrows, ncols=ncols) # Draw the predicted action\n",
    "print(axs.shape)\n",
    "\n",
    "for i in range(cfg.batch_size):\n",
    "    axs_row = int(i / nrows)\n",
    "    axs_col = int(i % nrows)\n",
    "    \n",
    "    action_np = dataset.denormalize_action(action[i].cpu().detach().numpy())\n",
    "    pred_action_np = dataset.denormalize_action(pred_action[i].cpu().detach().numpy())\n",
    "    \n",
    "    if cfg.pos_type == 'corners':\n",
    "        curr_pos_np = dataset.denormalize_corner(curr_pos[i].cpu().detach().numpy())\n",
    "        plot_corners(\n",
    "            ax=axs[axs_col, axs_row],\n",
    "            curr_pos=curr_pos_np,\n",
    "            use_img=False,\n",
    "            img=None,\n",
    "            plot_action=True,\n",
    "            actions=(action_np, pred_action_np))\n",
    "    elif cfg.pos_type == 'rvec_tvec':\n",
    "        curr_pos_np = dataset.denormalize_pos_rvec_tvec(curr_pos[i].cpu().detach().numpy())\n",
    "        plot_rvec_tvec(\n",
    "            ax=axs[axs_col, axs_row],\n",
    "            curr_pos=curr_pos_np,\n",
    "            use_img=False,\n",
    "            img=None,\n",
    "            plot_action=True,\n",
    "            actions=(action_np, pred_action_np))\n",
    "        \n",
    "plt.savefig(os.path.join(out_dir, 'pil_action_test.jpg'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c54392799b1ea06a5d9c28f64f7b4d3d25501cf92ff871f812783b9868cdd9b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
